# @package transformer_collective_network.optimizers.transformer_encoder
# GPU Server optimized transformer encoder optimizer

_target_: torch.optim.Adam
lr: 5e-4  # Slightly lower than actor/critic for stable transformer training
betas: [0.9, 0.999]
weight_decay: 1e-6  # Small weight decay for regularization