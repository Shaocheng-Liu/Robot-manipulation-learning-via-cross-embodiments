# @package _group_
train:
  [
      # The format is [key, abbr, type, average/constant]
      # If a key ends with "_", n (= number of envs),
      # copies of the metric are created.
    [episode, E, int, average],
    [step, S, int, average],
    [duration, D, time, average],
    [episode_reward, R, float, average],
    [success, Su, float, average],
    [batch_reward, BR, float, average],
    [actor_loss, ALOSS, float, average],
    [critic_loss, CLOSS, float, average],
    [ae_loss, RLOSS, float, average],
    [ae_transition_loss, null, float, average],
    [reward_loss, null, float, average],
    [actor_target_entropy, null, float, average],
    [actor_entropy, null, float, average],
    [alpha_loss, null, float, average],
    [alpha_value, null, float, average],
    [contrastive_loss, MLOSS, float, average],
    [max_rat, MR, float, average],
    [env_index, ENV, str, constant],
    [episode_reward_env_index_, R_, float, average],
    [success_env_index_, Su_, float, average],
    [env_index_, ENV_, str, constant],
    [batch_reward_agent_index_, null, float, average],
    [critic_loss_agent_index_, AGENT_, float, average],
    [actor_distilled_agent_loss_agent_index_, null, float, average],
    [actor_loss_agent_index_, null, float, average],
    [actor_target_entropy_agent_index_, null, float, average],
    [actor_entropy_agent_index_, null, float, average],
    [alpha_loss_agent_index_, null, float, average],
    [alpha_value_agent_index_, null, float, average],
    [ae_loss_agent_index_, null, float, average],
    # added metrics
    [mu_div, MUD, float, average],
    [log_std_div, STD, float, average],
    [kl_div, KLD, float, average],
    [vae_loss, VLOSS, float, average],
    [K_comps, DP_Comps, int, average],
    [kl_weight, KLW, float, average],
    [kl_weight_, KLW, float, average],
    [wm_duration, WM/DUR, time, average],
    [world_model_dynamics_loss, WM/DynLoss, float, average],
    [world_model_reward_loss, WM/RwdLoss, float, average],
    [world_model_total_loss, WM/Total, float, average],
    
  ]
eval:
  [
    [episode, E, int, average],
    [step, S, int, average],
    [episode_reward, R, float, average],
    [env_index, ENV, str, constant],
    [success, Su, float, average],
    [episode_reward_env_index_, R_, float, average],
    [success_env_index_, Su_, float, average],
    [env_index_, ENV_, str, constant],
    [batch_reward_agent_index_, AGENT_, float, average],
    [avg_dynamics_loss, WM/DynLoss, float, average],
    [avg_reward_loss, WM/RwdLoss, float, average],
    [avg_total_loss, WM/Total, float, average],
  ]
col_train:
  [
      # The format is [key, abbr, type, average/constant]
      # If a key ends with "_", n (= number of envs),
      # copies of the metric are created.
    [episode, E, int, average],
    [step, S, int, average],
    [duration, D, time, average],
    [episode_reward, R, float, average],
    [success, Su, float, average],
    [batch_reward, BR, float, average],
    [actor_loss, ALOSS, float, average],
    [critic_loss, CLOSS, float, average],
    [ae_loss, RLOSS, float, average],
    [ae_transition_loss, null, float, average],
    [reward_loss, null, float, average],
    [actor_target_entropy, null, float, average],
    [actor_entropy, null, float, average],
    [alpha_loss, null, float, average],
    [alpha_value, null, float, average],
    [contrastive_loss, MLOSS, float, average],
    [max_rat, MR, float, average],
    [env_index, ENV, str, constant],
    [episode_reward_env_index_, R_, float, average],
    [success_env_index_, Su_, float, average],
    [env_index_, ENV_, str, constant],
    [batch_reward_agent_index_, null, float, average],
    [critic_loss_agent_index_, AGENT_, float, average],
    [actor_distilled_agent_loss_agent_index_, null, float, average],
    [actor_loss_agent_index_, null, float, average],
    [actor_target_entropy_agent_index_, null, float, average],
    [actor_entropy_agent_index_, null, float, average],
    [alpha_loss_agent_index_, null, float, average],
    [alpha_value_agent_index_, null, float, average],
    [ae_loss_agent_index_, null, float, average],
    # added metrics
    [mu_div, MUD, float, average],
    [log_std_div, STD, float, average],
    [kl_div, KLD, float, average],
    [vae_loss, VLOSS, float, average],
    [K_comps, DP_Comps, int, average],
  ]
col_eval:
  [
    [episode, E, int, average],
    [step, S, int, average],
    [episode_reward, R, float, average],
    [env_index, ENV, str, constant],
    [success, Su, float, average],
    [episode_reward_env_index_, R_, float, average],
    [success_env_index_, Su_, float, average],
    [env_index_, ENV_, str, constant],
    [batch_reward_agent_index_, AGENT_, float, average],
    [critic_loss, CLOSS, float, average],
    [avg_dynamics_loss, WM/DynLoss, float, average],
    [avg_reward_loss, WM/RwdLoss, float, average],
    [avg_total_loss, WM/Total, float, average],
  ]
