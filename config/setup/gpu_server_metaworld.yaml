# @package _group_
# GPU Server optimized setup for MetaWorld experiments
# Use this configuration when training on multi-GPU servers (4x A100 or similar)

seed: 1
setup: metaworld
base_path:
save_dir: ${setup.base_path}/logs/experiment_gpu_server
device: cuda:0  # Primary GPU, multi-GPU support will be handled by torch.nn.DataParallel
id: gpu_server_experiment
description: GPU Server Optimized Training
tags:
  - gpu_server
  - multi_gpu
  - optimized
  - metaworld
git:
  commit_id:
  has_uncommitted_changes:
  issue_id:
date:
slurm_id:
debug:
  should_enable: False
metadata_path: ${project_root}/metadata/task_embedding/roberta_small/metaworld-all.json

# GPU Server specific optimizations
gpu_optimization:
  enable_multi_gpu: True
  mixed_precision: True  # Enable automatic mixed precision for faster training
  memory_optimization: True
  compile_model: False  # Set to True for PyTorch 2.0+ with torch.compile